{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PySpark\n",
    "\n",
    "### Lambda Functions\n",
    "Lambda functions are anonymous functions that are defined inline using the `lambda` keyword and limited to a single expression. A common use-case for lambda functions is to have small anonymous functions that maintain no external state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python', 'awesome', 'is', 'programming']\n"
     ]
    }
   ],
   "source": [
    "x = ['Python', 'programming', 'is', 'awesome']\n",
    "print(sorted(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['awesome', 'is', 'programming', 'Python']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(x, key=lambda arg: arg.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `filter()`, `map()`, and `reduce()`\n",
    "\n",
    "It is important to understand these functions in a core Python context. Then, you'll be able to translate the knowledge into PySpark programs and the Spark API.\n",
    "\n",
    "`filter` filters items out of an iterable based on a condition, typically expressed as a `lambda` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python', 'is', 'awesome']\n"
     ]
    }
   ],
   "source": [
    "# Using filter()\n",
    "print(list(filter(lambda arg: len(arg) < 8, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python', 'is', 'awesome']\n"
     ]
    }
   ],
   "source": [
    "# Much more verbose option\n",
    "def is_less_than_8_characters(item):\n",
    "    return len(item) < 8\n",
    "\n",
    "results = list()\n",
    "\n",
    "for item in x:\n",
    "    if is_less_than_8_characters(item):\n",
    "        results.append(item)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above collects all the strings that have less than 8 characters. The last code cell is more verbose than the `filter` example, but performs the same function with the results.\n",
    "\n",
    "Another less obvious benefit of `filter` is that it returns an iterable. This means `filter` doesn't require that your computer have enough memory to hold all the items in the iterable at once. This is increasingly important with big data sets that can quickly grow to several gigabytes in size.\n",
    "\n",
    "`map` is similar to `filter` in that it applies a function to each item in an iterable, but it always produces a 1-to-1 mapping of the original items. The new iterable that `map` returns will always have the same number of elements as the original iterable, whie was not the case with `filter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PYTHON', 'PROGRAMMING', 'IS', 'AWESOME']\n"
     ]
    }
   ],
   "source": [
    "# Using map()\n",
    "print(list(map(lambda arg: arg.upper(), x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PYTHON', 'PROGRAMMING', 'IS', 'AWESOME']\n"
     ]
    }
   ],
   "source": [
    "# Using an iterable\n",
    "results = list()\n",
    "for item in x:\n",
    "    results.append(item.upper())\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `for` loop has the same result as the `map` example, which collects all items in their upper-case form. However, as with the `filter` example, `map` returns an iterable, which again makes it possible to process large data that are too big to fit entirely in memory.\n",
    "\n",
    "`reduce` does not return an iterable. Instead, `reduce` uses the function called to reduce the iterable to a single value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python programming is awesome\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce # reduce() moved to functools in Python 3\n",
    "print(reduce(lambda val1, val2: val1 + ' ' + val2, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code combines all the items in the iterable, from left to right, into a single item. There is no call to `list` because `reduce` already returns a single item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Hello World\" in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "\n",
    "txt = sc.textFile('file:////usr/share/doc/python/copyright')\n",
    "print(txt.count())\n",
    "\n",
    "python_lines = txt.filter(lambda line: 'python' in line.lower())\n",
    "print(python_lines.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program counts the total number of lines and the number of lines that have the word `python` in a file named `copyright`.\n",
    "\n",
    "Remember, **a PySpark program is not that much different from a regular Python program**, but the **execution model can be very different** from a regular Python program, especially if it is running on a cluster.\n",
    "\n",
    "### What is Spark?\n",
    "\n",
    "Apache Spark is made up of several components - describing it can be difficult. At its core, Spark is a generic engine for processing large amounts of data.\n",
    "\n",
    "Spark is written in Scala and runs on the JVM. It has built-in components for processing streaming data, machine learning, graph processing, and even interacting with data via SQL.\n",
    "\n",
    "### What is PySpark?\n",
    "\n",
    "Spark is implemented in Scala, a programming language that runs on the JVM. To access all the functionality of Spark in Python you can use PySpark.\n",
    "\n",
    "Think of PySpark as a Python-based wrapper on top of the Scala API. This means we have to refer to two sets of documentation:\n",
    "\n",
    ">1. [PySpark API documentation](http://spark.apache.org/docs/latest/api/python/index.html)\n",
    ">2. [Spark Scala API documentation](https://spark.apache.org/docs/latest/api/scala/index.html#package)\n",
    "\n",
    "PySpark communicates with the Spark Scala-based API via the Py4J library. Py4J is not specific to PySpark or Spark. Py4J allows any Python program to talk to JVM-based code.\n",
    "\n",
    "Two reasons why PySpark is based on the the functional programming paradigm:\n",
    "\n",
    ">1. Spark's native language, Scala, is functional.\n",
    ">2. Functional code is much easier to parallelize.\n",
    "\n",
    "Another way to think of PySpark is that it is a library that allows processing of large amounts of data on a single machine or a cluster of machines.\n",
    "\n",
    "In a Python context, think of PySpark as a way to handle parallel processing without the need for the `threading` or `multiprocessing` modules. All the complicated communication and synchronization between threads, processes, and even different CPUs is handled by Spark.\n",
    "\n",
    "### PySpark API and Data Structures\n",
    "\n",
    "To interact with PySpark, we create specialized data structures called Resilient Distributed Datasets (RDDs).\n",
    "\n",
    "RDDs hide all the complexity of transforming and distributing our data automatically across multiple nodes by a scheduler if we running on a cluster.\n",
    "\n",
    "To better understand PySpark's API and data structures, recall the `Hello World` program above:\n",
    "\n",
    "```python\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "\n",
    "txt = sc.textFile('file:////usr/share/doc/python/copyright')\n",
    "print(txt.count())\n",
    "\n",
    "python_lines = txt.filter(lambda line: 'python' in line.lower())\n",
    "print(python_lines.count())\n",
    "```\n",
    "\n",
    "The entry-point of any PySpark program is a `SparkContext` object. This object allows us to connect to a Spark cluster and create RDDs. The `local[*]` string is a special string denoting that we are using a *local* cluster, which is another way of saying we are running in single-machine mode. The `*` tells Spark to creat as many worker threads as logical cores on our machine.\n",
    "\n",
    "Creating a `SparkContext` can be more involved when we are using a cluster. To connect to a Spark cluster, we might need to handle authentication and a few other pieces of information specific to our cluster. We can set up those details in the following manner:\n",
    "\n",
    "```python\n",
    "conf = pyspark.SparkConf()\n",
    "conf.setMaster('spark://head_node:56887')\n",
    "conf.set('spark.authenticate', True)\n",
    "conf.set('spark.authenticate.secret', 'secret-key')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "```\n",
    "\n",
    "You can only start creating RDDs once you have a `SparkContext`.\n",
    "\n",
    "RDDs can be created in a number of ways, but one common way is the PySpark `parallelize()` function. `parallelize` can transform some Python data structures like lists and tuples into RDDs, which gives you functionality that makes them fault-tolerant and distributed.\n",
    "\n",
    "To better understand RDDs, consider another example. The following code creates an iterator of 10K elements and then uses `parallelize` to distribute that data into 2 partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_list = range(10000)\n",
    "rdd = sc.parallelize(big_list, 2)\n",
    "odds = rdd.filter(lambda x: x % 2 != 0)\n",
    "odds.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`parallelize` turns that iterator into a distributed set of numbers and gives us all the capability of Spark's infrastructure.\n",
    "\n",
    "Note that we use the RDD's `filter` method instead of Python's built-in `filter`. The result is the same, but what is happening behind the scenes is drastically different. By using the RDD `filter` method, that operation occurs in a distributed manner across several CPUs or computers.\n",
    "\n",
    "Again, imagine this is Spark doing the `multiprocessing` work all encapsulated in the RDD data structure.\n",
    "\n",
    "`take()` is a way to see the contents of our RDD, but only as small subset. `take` pulls that subset of data from the distributed system onto a single machine.\n",
    "\n",
    "`take()` is important for debugging because inspecting our entire dataset on a single machine may not be possible. RDDs are optimised to be used on big data so in a real world scenario, a single machine may not have enough RAM to hold an entire dataset.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Spark temporarily prints information to stdout when running examples like this in the shell. Your stdout might temporarily show something like [Stage 0:> (0 + 1) / 1].\n",
    "\n",
    "The stdout text demonstrates how Spark is splitting up the RDDs and processing your data into multiple stages across different CPUs and machines.\n",
    "</b>\n",
    "</div>\n",
    "\n",
    "\n",
    "Another way to create RDDs is to read the file with `textFile()`. RDDs are one of the foundational data structures for using PySpark so many of the functions in the API return RDDs.\n",
    "\n",
    "One of the key distinctions between RDDs and other data structures is that processing is delayed until the result is requested. This is similar to a Python generator. Developers in the Python ecosystem typically use the term lazy evaluation to explain this behavior.\n",
    "\n",
    "You can stack up multiple transformations on the same RDD without any processing happening. This functionality is possible because Spark maintains a directed acyclic graph (DAG) of the transformations. The underlying graph is only activated when the final results are requested. In the previous example, no computation took place until we requested the results by calling `take()`.\n",
    "\n",
    "There are multiple ways to request the results from an RDD. You can explicitly request results to be evaluated and collected to a single cluster node by using `collect()` on a RDD. You can also implicitly request the results in various ways, one of which is using `count()`.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>\n",
    "Be careful when using these methods because they pull the entire dataset into memory, which will not work if the dataset is too big to fit into the RAM of a single machine.\n",
    "</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "> [First Steps with PySpark and Big Data Processing](https://realpython.com/pyspark-intro/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
